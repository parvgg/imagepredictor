# -*- coding: utf-8 -*-
"""imageprediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gkCv3n_Z99Fjj5m3u3socPx_t2w52ymL
"""

import tensorflow as tf
import numpy as np

from tensorflow import keras

batch_size = 32
image_height=100
image_width=100

data_dir='/content/drive/MyDrive/Colab Notebooks/Dataset'

train_ds = tf.keras.utils.image_dataset_from_directory(data_dir,validation_split=0.2,subset="training",
  seed=123,image_size=(image_height, image_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  data_dir,
  validation_split=0.2,
  subset="validation",
  seed=123,
  image_size=(image_height, image_width),
  batch_size=batch_size)

classes=train_ds.class_names

classes

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(classes[labels[i]])
    plt.axis("off")

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

autotune=tf.data.AUTOTUNE  #finetuning

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=autotune)
val_ds = val_ds.cache().prefetch(buffer_size=autotune)

from keras import layers

normalisation_layer=layers.Rescaling(1./255)
normalised_ds=train_ds.map(lambda x,y: (normalisation_layer(x),y))
image_batch, labels_batch = next(iter(normalised_ds))   #can consider 1 line loop

first_image=image_batch[0]

print(np.min(first_image),np.max(first_image))

from keras.models import Sequential
model = Sequential([
    layers.Rescaling(1.255, input_shape = (image_height, image_width,3)),
    layers.Conv2D(16,3,padding='same', activation= 'relu'),
    layers.MaxPooling2D(),  #reduces spatial dimension
    layers.Conv2D(16,3,padding='same', activation= 'relu'),
    layers.MaxPooling2D(),  #reduces spatial dimension
    layers.Conv2D(16,3,padding='same', activation= 'relu'),
    layers.MaxPooling2D(),  #reduces spatial dimension
    layers.Flatten(),
    layers.Dense(128, activation='relu'), #hidden layer end
    layers.Dense(len(classes))
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

epochs = 10
history = model.fit(train_ds, validation_data=val_ds, epochs = epochs)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

#the above data trained is good as can be seen from the graph but if the distance between lines is large it might be overfitting
data_aug = Sequential([layers.RandomFlip("horizontal",input_shape = (image_height, image_width,3)),
                       layers.RandomRotation(0.1),
                       layers.RandomZoom(0.1)])

plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(9):
    augmented_images = data_aug(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")

model = Sequential([
  data_aug,
  layers.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(128, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(256, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.2), #to find condition of over fitting
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(len(classes), name="outputs")
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

history = model.fit(train_ds, validation_data=val_ds, epochs=15)

data_url = "https://5.imimg.com/data5/SR/IS/MY-5751538/neem-leaves.jpg"
data_path = tf.keras.utils.get_file('Plant', origin = data_url)

img = tf.keras.utils.load_img(
    data_path, target_size=(image_height, image_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print(
    "This image most likely belongs to {} with a {:.2f} percent confidence."
    .format(classes[np.argmax(score)], 100 * np.max(score))
)

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)